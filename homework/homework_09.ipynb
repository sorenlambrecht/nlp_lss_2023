{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Make the GPT-2 simulate republican cases through Reinforcement Learning\n",
        "\n",
        "In this homework, we will first train a reward model that assign higher reward to documents that sounds more like republican cases. Then we use RL to guide GPT-2 to complete the democratric cases in a republican way.\n",
        "\n",
        "The reward model is covered in a previous notebook. All TODOs you need to finish lie in the RL part."
      ],
      "metadata": {
        "id": "DfJ5miNsCxkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install transformers trl"
      ],
      "metadata": {
        "id": "Ck4Jy5RCuz2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0brYiTVr8HVb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "bbb524b8-c4e2-4964-da4e-9c3f3ef54e11"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-259acca9-b365-47f1-9549-f52a33ab77a8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-259acca9-b365-47f1-9549-f52a33ab77a8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving sc_cases_cleaned.pkl to sc_cases_cleaned.pkl\n"
          ]
        }
      ],
      "source": [
        "# load sc_cases_cleaned.pkl that we used in the previous notebooks\n",
        "# can be also find in https://github.com/elliottash/nlp_lss_2023/blob/master/notebooks/sc_cases_cleaned.pkl\n",
        "from google.colab import files                                                                                                                                                                                       \n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings; warnings.simplefilter('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from trl.core import LengthSampler\n"
      ],
      "metadata": {
        "id": "pWAg2_MfAHJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train a classification model as our reward model"
      ],
      "metadata": {
        "id": "rWSxGM4OvKcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_pickle('sc_cases_cleaned.pkl', compression='gzip')\n",
        "df = df.assign(author_id=(df['authorship']).astype('category').cat.codes)\n",
        "\n",
        "# gpu or cpu?\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print (device)\n",
        "\n",
        "model_name = 'distilbert-base-uncased' # huggingface model_ID or path to folder \n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
        "inputs = tokenizer(df['opinion_text'].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
        "labels = torch.tensor(df['x_republican'].tolist()).long()\n",
        "\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': model.distilbert.parameters(), 'lr': 1e-5},  \n",
        "    {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
        "])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['opinion_text'].tolist(), df['x_republican'].tolist(), test_size=.2)\n",
        "\n",
        "# generate batches\n",
        "X_train, X_test, y_train, y_test = np.array(X_train[:608]), np.array(X_test[:152]), np.array(y_train[:608]), np.array(y_test[:152])\n",
        "print (X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = X_train.reshape(-1, 8), X_test.reshape(-1, 8), y_train.reshape(-1, 8), y_test.reshape(-1, 8)\n",
        "print (X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "X_train, X_test = X_train.tolist(), X_test.tolist()\n",
        "\n",
        "# train\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.to(device)\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for text, labels in tqdm(zip(X_train, y_train), total=len(X_train)):\n",
        "        # prepare model input through our tokenizer\n",
        "        model_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
        "        # place everything on the right device\n",
        "        model_inputs = {k:v.to(device) for k,v in model_inputs.items()}\n",
        "        # labels have to be torch long tensors\n",
        "        labels = torch.tensor(labels).long().to(device)\n",
        "        # now, we can perform the forward pass\n",
        "        output = model(**model_inputs, labels=labels)\n",
        "        loss, logits = output[:2]\n",
        "        # and the backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "torch.save(model, 'republican_classifier.pt')\n",
        "republican_classifier = model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aukDifLTu9T2",
        "outputId": "b8da5ae6-5663-4ec9-e30e-73a0f2c9c593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(608,) (152,) (608,) (152,)\n",
            "(76, 8) (19, 8) (76, 8) (19, 8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:16<00:00,  4.62it/s]\n",
            "100%|██████████| 76/76 [00:15<00:00,  4.81it/s]\n",
            "100%|██████████| 76/76 [00:16<00:00,  4.67it/s]\n",
            "100%|██████████| 76/76 [00:16<00:00,  4.62it/s]\n",
            "100%|██████████| 76/76 [00:16<00:00,  4.68it/s]\n",
            "100%|██████████| 76/76 [00:22<00:00,  3.40it/s]\n",
            "100%|██████████| 76/76 [00:15<00:00,  4.75it/s]\n",
            "100%|██████████| 76/76 [00:16<00:00,  4.71it/s]\n",
            "100%|██████████| 76/76 [00:21<00:00,  3.59it/s]\n",
            "100%|██████████| 76/76 [00:18<00:00,  4.09it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, TextClassificationPipeline\n",
        "\n",
        "distil_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', max_length=512, truncation=True)\n",
        "pipeline = TextClassificationPipeline(model=republican_classifier, tokenizer=distil_tokenizer, return_all_scores=True, device=republican_classifier.device)"
      ],
      "metadata": {
        "id": "sXmvfL3F9g43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df.loc[df['x_republican'] == 0, 'opinion_text'].tolist()\n",
        "texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "wsMBBD1R_sdb",
        "outputId": "3f4f6a2a-5e22-447f-ee49-a1d7f2bb58a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'JUSTICE GINSBURG delivered the opinion of the Court.\\n\\n A motion by a federal prisoner for postconviction relief under 28 U.S.C. § 2255 is subject to a one-year time limitation that generally runs from \"the date on which the judgment of conviction becomes final.\" § 2255, P6(1). This case concerns the starting date for the one-year limitation. It presents a narrow but recurring question on which courts of appeals have divided: When a defendant in a federal prosecution takes an unsuccessful direct appeal from a judgment of conviction, but does not next petition for a writ of certiorari from this Court, does the judgment become \"final\" for postconviction relief purposes (1) when the appellate court issues its mandate affirming the conviction, or, instead, (2) on the date, ordinarily 69 days later, when the time for filing a petition for certiorari expires?\\n\\nIn accord with this Court\\'s consistent understanding of finality in the context of collateral review, and the weight of lower court authority, we reject the issuance of the appellate  court mandate as the triggering date. For the purpose of starting the clock  on § 2255\\'s one-year limitation period, we hold, a judgment of conviction becomes final when the time expires for filing a petition for certiorari contesting the appellate court\\'s affirmation of the conviction.\\n\\nI\\n\\nIn 1997, petitioner Erick Cornell Clay was convicted of arson and distribution of cocaine base in the United States District Court for the Northern District of Indiana. On November 23, 1998, the Court of Appeals for the Seventh Circuit affirmed his convictions. That court\\'s mandate issued on December 15, 1998. See Fed. Rules App. Proc. 40(a)(1) and 41(b) (when no petition for rehearing is filed, a court of appeals\\' mandate issues 21 days after entry of judgment). Clay did not file a petition for a writ of certiorari. The time in which he could have petitioned for certiorari expired on February 22, 1999, 90 days after entry of the Court of Appeals\\' judgment, see this Court\\'s Rule 13(1), and 69 days after the issuance of the appellate court\\'s mandate.\\n\\n On February 22, 2000 -- one year and 69 days after the Court of Appeals issued its mandate and exactly one year after the time for seeking certiorari expired -- Clay filed a motion in the District Court, pursuant to 28 U.S.C. § 2255, to vacate, set aside, or correct his sentence. Congress has prescribed \"[a] 1-year period of limitation\" for such motions \"running from the latest of\" four specified dates. § 2255, P6. Of the four dates, the only one relevant in this case, as in the generality of cases, is the first: \"the date on which the judgment of conviction becomes final.\" § 2255, P6(1).\\n\\nRelying on Gendron v. United States, 154 F.3d 672, 674 (CA7 1998)(per curiam), the District Court stated that \"when a federal prisoner in this circuit does not seek certiorari . . ., the conviction becomes \\'final\\' on the date the appellate court issues the mandate in the direct appeal.\" App. to Pet. for Cert.  8a. Because Clay filed his § 2255 motion  more than one year after that date, the court denied the motion as time barred.\\n\\nThe Seventh Circuit affirmed. That court declined Clay\\'s \"invitation to reconsider our holding in Gendron,\" although it acknowledged that Gendron\\'s \"construction of section 2255 represents the minority view.\" 30 Fed. Appx. 607, 609 (2002). \"Bowing to stare decisis,\" the court expressed \"reluctance to overrule [its own] recently-reaffirmed precedent without guidance from the Supreme Court.\" Ibid.\\n\\n The Fourth Circuit has agreed with Gendron\\'s interpretation of § 2255. See United States v. Torres, 211 F.3d 836, 838-842 (2000) (when a federal prisoner does not  file a petition for certiorari, his judgment of conviction becomes final for § 2255 purposes upon issuance of the court of appeals\\' mandate). Six Courts of Appeals have parted ways with the Seventh and Fourth Circuits. These courts hold that, for federal prisoners like Clay who do not file petitions for certiorari following affirmance of their convictions, § 2255\\'s one-year limitation period begins to run when the defendant\\'s time for seeking review by this Court expires. 1See Derman v. United States, 298 F.3d 34, 39-42 (CA1 2002); Kapral v. United States, 166 F.3d 565, 567-577 (CA3 1999); United States v. Gamble, 208 F.3d 536, 537 (CA5 2000)(per curiam); United States v. Garcia, 210 F.3d 1058, 1059-1061 (CA9 2000); United States v. Burch, 202 F.3d 1274, 1275-1279 (CA10 2000); Kaufmann v. United States, 282 F.3d 1336, 1337-1339 (CA11 2002). To  secure uniformity in the application of § 2255\\'s time constraint, we granted certiorari, 536 U.S. 957, 153 L. Ed. 2d 834, 122 S. Ct. 2658 (2002), and now reverse the Seventh Circuit\\'s judgment. 2Agreeing with the position advanced by the majority of the courts of appeals that have ruled on the question, the United States joins petitioner Clay in urging that Clay\\'s § 2255 motion was timely filed. We therefore invited David W. DeBruin to brief and argue this case, as amicus curiae, in support of the Seventh Circuit\\'s judgment. Mr. DeBruin\\'s able advocacy permits us to decide the case satisfied that the relevant issues have been fully aired.\\n\\nIIFinality is variously defined; like many legal terms, its precise meaning depends on context. Typically, a federal judgment becomes final for appellate review and claim preclusion purposes when the district court disassociates itself from the case, leaving nothing to be done at the court of first instance save execution of the judgment. See, e.g., Quackenbush v. Allstate Ins. Co.,517 U.S. 706, 712, 135 L. Ed. 2d 1, 116 S. Ct. 1712 (1996); Restatement (Second) of Judgments § 13, Comment b (1980). For other purposes, finality attaches at a different stage. For example, for certain determinations under the Speedy Trial Act of 1974, 18 U.S.C. § 3161 et seq., and under a now-repealed version of Federal Rule of Criminal Procedure 33, several lower courts have held that finality attends issuance of the appellate court\\'s mandate. See Brief for Amicus Curiae by Invitation of the Court 22-28 (hereinafter DeBruin Brief) (citing cases). For the purpose of seeking review by this Court, in contrast, \"the time to file a petition for a writ of certiorari runs from the date of entry of the judgment or order sought to be reviewed, and not from the issuance date of the mandate (or its equivalent under local practice).\" This Court\\'s Rule 13(3).\\n\\nHere, the relevant context is postconviction relief, a context in which finality has a long-recognized, clear meaning: Finality attaches when this Court affirms a conviction on the merits on direct review or denies a petition for a writ of certiorari, or when the time for filing a certiorari petition expires. See, e.g., Caspari v. Bohlen,510 U.S. 383, 390, 127 L. Ed. 2d 236, 114 S. Ct. 948 (1994);Griffith v. Kentucky,479 U.S. 314, 321, n. 6, 93 L. Ed. 2d 649, 107 S. Ct. 708 (1987);Barefoot v. Estelle,463 U.S. 880, 887, 77 L. Ed. 2d 1090, 103 S. Ct. 3383 (1983);United States v. Johnson,457 U.S. 537, 542, n. 8, 73 L. Ed. 2d 202, 102 S. Ct. 2579 (1982);Linkletter v. Walker,381 U.S. 618, 622, n. 5, 14 L. Ed. 2d 601, 85 S. Ct. 1731 (1965). Because \"we presume that Congress expects its statutes to be read in conformity with this Court\\'s precedents,\" United States v. Wells,519 U.S. 482, 495, 137 L. Ed. 2d 107, 117 S. Ct. 921 (1997), our unvarying understanding  of finality for collateral review purposes would ordinarily determine the meaning of \"becomes final\" in § 2255.Amicus urges a different determinant, relying on verbal differences between § 2255 and a parallel statutory provision, 28 U.S.C. § 2244(d)(1), which governs petitions for federal habeas corpus by state prisoners. See  DeBruin Brief 8-20. Sections 2255 and 2244(d)(1), as now formulated, were reshaped by the Antiterrorism and Effective Death Penalty Act of 1996. See §§ 101, 105, 110 Stat. 1217, 1220. Prior to that Act, no statute of limitations governed requests for federal habeas corpus or § 2255 habeas-like relief. See Vasquez v. Hillery,474 U.S. 254, 265, 88 L. Ed. 2d 598, 106 S. Ct. 617 (1986); United States v. Nahodil, 36 F.3d 323, 328 (CA3 1994). Like § 2255, § 2244(d)(1) establishes a one-year limitation period, running from the latest of four specified dates. Three of the four time triggers under § 2244(d)(1) closely track corresponding portions of § 2255. Compare §§ 2244(d)(1)(B)-(D), with § 2255, PP6(2)-(4). But where § 2255, P6(1), refers simply to \"the date on which the judgment of conviction becomes final,\" § 2244(d)(1)(A) speaks of \"the date on which the judgment became final by the conclusion of direct review or the expiration of the time for seeking such review.\" 3The Courts of Appeals have uniformly interpreted \"direct review\" in § 2244(d)(1)(A) to encompass review of a state conviction by this Court. See Derman v. United States, 298 F.3d at 40-41; Williams v. Artuz, 237 F.3d 147, 151 (CA2 2001); Kapral v. United States, 166 F.3d at 575; Hill v. Braxton, 277 F.3d 701, 704 (CA4 2002); Ott v. Johnson, 192 F.3d 510, 513 (CA5 1999); Bronaugh v. Ohio, 235 F.3d 280, 283 (CA6 2000); Anderson v. Litscher, 281 F.3d 672, 674-675 (CA7 2002); Smith v. Bowersox, 159 F.3d 345, 347-348 (CA8 1998); Bowen v. Roe, 188 F.3d 1157, 1159 (CA9 1999); Locke v. Saffle, 237 F.3d 1269, 1273 (CA10 2001); Bond v. Moore, 309 F.3d 770, 774 (CA11 2002).\\n\\nWhen \"Congress includes particular language in one section of a statute but omits it in another section of the same Act,\" we have recognized, \"it is generally presumed that Congress acts intentionally and purposely in the disparate inclusion or exclusion.\" Russello v. United States,464 U.S. 16, 23, 78 L. Ed. 2d 17, 104 S. Ct. 296 (1983) (quoting United States v. Wong Kim Bo, 472 F.2d 720, 722 (CA5 1972)). Invoking the maxim recited in Russello, amicus asserts that \"becomes final\" in § 2255, P6(1), cannot mean the same thing as \"became final\" in § 2244(d)(1)(A); reading the two as synonymous, amicus maintains, would render superfluous the words \"by the conclusion of direct review or the expiration of the time for seeking such review\" -- words found only in the latter provision. DeBruin Brief 8-20. We can give effect to the discrete wording of the two prescriptions, amicus urges, if we adopt the following rule: When a convicted defendant does not seek certiorari on direct review, § 2255\\'s limitation period starts to run on the date the court of appeals issues its mandate. Id., at 36. 4Although recognizing that \"the question is not presented in this case,\" Tr. of Oral Arg. 27, amicus suggests that § 2255\\'s limitation period starts to run upon issuance of the court of appeals\\' mandate even in cases in which the defendant does petition for certiorari. Id., at 27-28, 36-38, 41-42. As amicus also recognizes, however, id., at 41, courts of appeals \"have uniformly concluded that, if a prisoner petitions for certiorari, the contested conviction becomes final when the Supreme Court either denies the writ or issues a decision on the merits,\" United States v. Hicks, 350 U.S. App. D.C. 279, 283 F.3d 380, 387 (CADC 2002).\\n\\nAmicus would have a stronger argument if § 2255, P6(1), explicitly incorporated the first of § 2244(d)(1)(A)\\'s finality formulations but not the second, so that the § 2255 text read \"becomes final by the conclusion of direct review.\" Had § 2255 explicitly provided for the first of the two finality triggers set forth in § 2244(d)(1)(A), one might indeed question the soundness of interpreting § 2255 implicitly to incorporate § 2244(d)(1)(A)\\'s second trigger as well. As written, however, § 2255 does not qualify \"becomes final\" at all. Using neither of the disjunctive phrases that follow the words \"became final\" in § 2244(d)(1)(A), § 2255 simply leaves \"becomes final\" undefined.\\n\\nRussello, we think it plain, hardly warrants the decision amicus urges, one that  would hold the § 2255 petitioner to  a tighter time constraint than the petitioner governed by § 2244(d)(1)(A). Russello concerned the meaning of a provision in the Racketeer Influenced and Corrupt Organizations Act (RICO), 18 U.S.C. § 1961 et seq., that directed forfeiture to the United States of \"any interest [a convicted defendant] has acquired . . . in violation of [the Act].\" § 1963(a)(1). The petitioner in Russello urged a narrow construction of the unqualified words \"any interest . . . acquired.\" Rejecting that argument, we observed that a succeeding subsection, § 1963(a)(2), reached \"any interest in . . . any enterprise\" the defendant conducted in violation of RICO\\'s proscriptions. At that point, we referred to the maxim invoked by amicus. See supra, at 6. The qualifying words \"in . . . any enterprise\" narrowed § 1963(a)(2), but in no way affected § 1963(a)(1). The comparison of the two subsections, we said, \"fortified\" the broad construction we approved for the unmodified words \"any interest . . . acquired.\" Russello, 464 U.S., at 22-23 (internal quotation marks omitted); see id., at 23 (\"Had Congress intended to restrict § 1963(a)(1) to an interest in an enterprise, it presumably would have done so expressly as it did in the immediately following subsection (a)(2).\").\\n\\nFar from supporting the Seventh Circuit\\'s constricted reading of § 2255, P6(1), Russello\\'s reasoning tends in Clay\\'s favor. An unqualified term -- here \"becomes final\" -- Russello indicates, calls for a reading surely no less broad than a pinpointed one -- here, § 2244(d)(1)(A)\\'s specification \"became final by the conclusion of direct review or the expiration of the time for seeking such review.\"\\n\\nMoreover, as Clay and the Government urge, see Brief for Petitioner 22; Reply Brief for United States 7-8, one can readily comprehend why Congress might have found it appropriate to spell out the meaning of \"final\" in § 2244(d)(1)(A) but not in § 2255. Section 2244(d)(1) governs petitions by state prisoners. In that context, a bare reference to \"became final\" might have suggested that finality assessments  should be made by reference to state law rules that may differ from the general federal rule and vary from State to State. Cf.  Artuz v. Bennett,531 U.S. 4, 8, 148 L. Ed. 2d 213, 121 S. Ct. 361 (2000) (an application for state postconviction relief is \"properly filed\" for purposes of 28 U.S.C. § 2244(d)(2) \"when its delivery and acceptance are in compliance with the applicable [state] laws and rules governing filings\"). The words \"by the conclusion of direct review or the expiration of the time for seeking such review\" make it clear that finality for the purpose of § 2244(d)(1)(A) is to be determined by reference to a uniform federal rule. Section 2255, however, governs only petitions by federal prisoners; within the federal  system there is no comparable risk of varying rules to guard against.\\n\\nAmicus also submits that 28 U.S.C. § 2263 \"reinforces\" the Seventh Circuit\\'s understanding of § 2255. DeBruin Brief 20; accord, Torres, 211 F.3d at 840. Chapter 154 of Title 28 governs certain habeas petitions filed by death-sentenced state prisoners. Section 2263(a) prescribes a 180-day limitation period for such petitions running from \"final State court affirmance of the conviction and sentence on direct review or the expiration of the time for seeking such review.\" That period is tolled, however, \"from the date that a petition for certiorari is filed in the Supreme Court until the date of final disposition of the petition if a State prisoner files the petition to secure review by the Supreme Court of the affirmance of a capital sentence on direct review by the court of last resort of the State or other final State court decision on direct review.\" § 2263(b)(1).\\n\\nWe do not find in § 2263 cause to alter our reading of § 2255. First, amicus\\' reliance on § 2263 encounters essentially the same problem as does his reliance on § 2244(d)(1)(A): Section 2255, P6(1), refers to neither of the two events that § 2263(a) identifies as possible starting points for the limitation period -- \"affirmance of the conviction and sentence on direct review\" and \"the expiration of the time for seeking such review.\" Thus, reasoning by negative implication from § 2263 does not justify the conclusion that § 2255, P6(1)\\'s limitation period begins to run at one of those times rather than the other. Cf. supra, at 6-8. Second, § 2263(a) ties the applicable limitation period to \"affirmance of the conviction and sentence,\" while § 2255, P6(1), ties the limitation period to the date when \"the judgment of conviction becomes final.\" See Torres, 211 F.3d at 845 (Hamilton, J., dissenting). \"The Russello presumption -- that the presence of a phrase in one provision and its absence in another reveals Congress\\' design -- grows weaker with each difference in the formulation of the provisions under inspection.\" Columbus v. Ours Garage & Wrecker Service, Inc.,536 U.S. 424, 435-436, 153 L. Ed. 2d 430, 122 S. Ct. 2226 (2002).\\n\\n* * *\\n\\nWe hold that, for federal criminal defendants who do not file a petition for certiorari with this Court on direct review, § 2255\\'s one-year limitation period starts to run when the time for seeking such review expires. Under this rule, Clay\\'s § 2255 petition was timely filed. The judgment of the United States Court of Appeals for the Seventh Circuit is therefore reversed, and the case is remanded for further proceedings consistent with this opinion.\\n\\nIt is so ordered.  \\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(X_test[0][0], truncation=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXNrGVTG9pBB",
        "outputId": "7ebd90ae-fef8-4bc9-f2f3-1b32c6560293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'label': 'LABEL_0', 'score': 2.4655144443386234e-05},\n",
              "  {'label': 'LABEL_1', 'score': 0.999975323677063}]]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reinforcement Learning"
      ],
      "metadata": {
        "id": "DgJufbVovTDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the dataset for reinforcement learning\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class PPODataset(Dataset):\n",
        "  def __init__(self, tokenizer, texts, input_min_text_length, input_max_text_length):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.texts = texts\n",
        "    self.random_sample = LengthSampler(input_min_text_length, input_max_text_length)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    text = self.texts[index]\n",
        "    sample = {}\n",
        "    sample[\"input_ids\"] = torch.tensor(tokenizer.encode(text)[: self.random_sample()])\n",
        "    sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
        "    return sample\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "dataset = PPODataset(tokenizer, texts, 10, 15)\n"
      ],
      "metadata": {
        "id": "LyspW26sChWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare arguments and configurations for RL\n",
        "\n",
        "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}\n",
        "\n",
        "output_min_length = 50\n",
        "output_max_length = 100\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"min_length\": -1,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id,\n",
        "}\n",
        "\n",
        "def collator(data):\n",
        "  return dict((key, [d[key] for d in data]) for key in data[0])\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name=\"gpt2\",\n",
        "    learning_rate=1.41e-5,\n",
        "    batch_size=32,\n",
        ")"
      ],
      "metadata": {
        "id": "R7sxSX44a_p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: prepare model (use gpt2), reference model, and PPO trainer for RL"
      ],
      "metadata": {
        "id": "d2ndwrUXGN1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: conduct PPO training loop here. For efficiency, you can just train 3 batches."
      ],
      "metadata": {
        "id": "R0iXvrJ-GD4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the outcomes generated by the RL tuned GPT-2\n",
        "\n",
        "#### get a batch from the dataset\n",
        "game_data = dict()\n",
        "game_data[\"query\"] = [dataset[i][\"query\"] for i in range(15)]\n",
        "query_tensors = [dataset[i][\"input_ids\"] for i in range(15)]\n",
        "\n",
        "response_tensors_ref, response_tensors = [], []\n",
        "gen_kwargs = {\"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id}\n",
        "#### get response from gpt2 and gpt2_ref\n",
        "for i in range(15):\n",
        "    gen_len = output_length_sampler()\n",
        "    output = ref_model.generate(\n",
        "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
        "    ).squeeze()[-gen_len:]\n",
        "    response_tensors_ref.append(output)\n",
        "    output = model.generate(\n",
        "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
        "    ).squeeze()[-gen_len:]\n",
        "    response_tensors.append(output)\n",
        "\n",
        "#### decode responses\n",
        "game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(15)]\n",
        "game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i]) for i in range(15)]\n",
        "\n",
        "#### sentiment analysis of query/response pairs before/after\n",
        "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
        "game_data[\"rewards (before)\"] = [output[1][\"score\"] for output in pipeline(texts, **sent_kwargs)]\n",
        "\n",
        "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
        "game_data[\"rewards (after)\"] = [output[1][\"score\"] for output in pipeline(texts, **sent_kwargs)]\n",
        "\n",
        "# store results in a dataframe\n",
        "df_results = pd.DataFrame(game_data)\n",
        "df_results"
      ],
      "metadata": {
        "id": "_3F_RUHkOW0p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}