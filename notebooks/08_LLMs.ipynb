{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Implement a PicoGPT from Scratch\n",
        "This notebook is adapted from https://github.com/jaymody/picoGPT\n",
        "\n",
        "Also see the blog: https://jaykmody.com/blog/gpt-from-scratch\n",
        "\n",
        "Important Note: implementations in this tutorial is not efficient at all! They are just explicit enough so that we can understand the ideas behind."
      ],
      "metadata": {
        "id": "S5filp-bg4ka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Building Blocks for a Causal Language Model"
      ],
      "metadata": {
        "id": "9fLtnQcbo7dA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Non-linearity for GPT-2: GELU\n",
        "It is approximated by the following function:"
      ],
      "metadata": {
        "id": "wHJaRC5bhpnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "gMQBVFTZwed9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUVGwnhPgyxb"
      },
      "outputs": [],
      "source": [
        "def gelu(x):\n",
        "  return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like ReLU, GELU operates element-wise on the input:"
      ],
      "metadata": {
        "id": "-EdG-sgrkVDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gelu(np.array([[1, 2], [-2, 0.5]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FccYMWbkUMe",
        "outputId": "7743e8f2-dbbb-4e0d-90bf-21c308a371eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.84119199,  1.95459769],\n",
              "       [-0.04540231,  0.34571401]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Softmax\n",
        "\n",
        "\\begin{align}\n",
        "\\text{softmax}(x)_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
        "    \\end{align}\n",
        "\n",
        "We use the max(x) trick for numerical stability (intuition: softmax computes natural exponents in its formula, which can easily overflow the upper limit of float point numbers. Therefore, we normalize each exponents by their maximum.)"
      ],
      "metadata": {
        "id": "RPwC2wQUkhOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "  return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "hnliEPelkjtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax(np.array([[2, 100], [-5, 0]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25hyrMSYmCKD",
        "outputId": "d3e4da7e-e238-438d-cfcd-f374c9976b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.74878501e-43, 1.00000000e+00],\n",
              "       [6.69285092e-03, 9.93307149e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Layer Normalization\n",
        "\\begin{align}\n",
        "\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2}} + \\beta\n",
        "    \\end{align}\n",
        "\n",
        "where $\\mu$ is the mean of $x$, $σ$ is the standard deviation of $x$\n",
        ", and $γ$ \n",
        " and $\\beta$\n",
        " are learnable parameters."
      ],
      "metadata": {
        "id": "KttXsfdgmHpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_norm(x, g, b, eps: float = 1e-5):\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    variance = np.var(x, axis=-1, keepdims=True)\n",
        "    x = (x - mean) / np.sqrt(variance + eps)  # normalize x to have mean=0 and var=1 over last axis\n",
        "    return g * x + b  # scale and offset with gamma/beta params"
      ],
      "metadata": {
        "id": "DaMjfX3ymMYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply layer normalization over the last axis of the input."
      ],
      "metadata": {
        "id": "I8rDmV23n69j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[2, 2, 3], [-5, 0, 1]])\n",
        "layer_norm(x, g=np.ones(x.shape[-1]), b=np.zeros(x.shape[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7qXd0Ddn6ak",
        "outputId": "62f02a3a-f915-48d5-d858-0ce139dec7b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.70709087, -0.70709087,  1.41418174],\n",
              "       [-1.39700038,  0.50800014,  0.88900024]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear\n",
        "Your standard matrix multiplication + bias:"
      ],
      "metadata": {
        "id": "g-OHEdAjopIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear(x, w, b):  # [m, in], [in, out], [out] -> [m, out]\n",
        "  return x @ w + b"
      ],
      "metadata": {
        "id": "PS1kjNpsosl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT Architecture\n",
        "Hooray! With all the building blocks, we can stack them to get our transformer decoder.\n",
        "<div style=\"text-align: center;\">\n",
        "<img src='https://i.imgur.com/c4Z6PG8.png' width='200'>\n",
        "</div>"
      ],
      "metadata": {
        "id": "YDOWHm0Yo2xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):  # [n_seq] -> [n_seq, n_vocab]\n",
        "    # token + positional embeddings\n",
        "    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n",
        "\n",
        "    # forward pass through n_layer transformer blocks\n",
        "    for block in blocks:\n",
        "      x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # projection to vocab\n",
        "    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]"
      ],
      "metadata": {
        "id": "k7Xq8fEwrv8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting takeaways from this high-level structure:\n",
        "\n",
        "1.   An extra layer-norm after transformer blocks: this is special in GPT-2 compared to the original transformer.\n",
        "2.   When projecting outputs to the vocabular space, wte is reused instead of learning a new one.\n",
        "\n"
      ],
      "metadata": {
        "id": "0BDfbBVUu41g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder Block"
      ],
      "metadata": {
        "id": "iuMlqGo2wLo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    # multi-head causal self attention\n",
        "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # position-wise feed forward network\n",
        "    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "def ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    # project up\n",
        "    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
        "\n",
        "    # project back down\n",
        "    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "xMAWjPOzwJWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multi-head Causal Self-attention\n",
        "\n",
        "##### Attention\n",
        "\\begin{align}\n",
        "\\text{attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "    \\end{align}"
      ],
      "metadata": {
        "id": "PIorxXTWx1kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(q, k, v):  # [n_q, d_k], [n_k, d_k], [n_k, d_v] -> [n_q, d_v]\n",
        "    return softmax(q @ k.T / np.sqrt(q.shape[-1])) @ v"
      ],
      "metadata": {
        "id": "N7NmVlG8x6Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Self"
      ],
      "metadata": {
        "id": "ksWURc5CyQP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_attention_1(x): # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    return attention(q=x, k=x, v=x)\n",
        "\n",
        "# Add linear projections\n",
        "def self_attention_2(x, w_k, w_q, w_v, w_proj): # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    # qkv projections\n",
        "    q = x @ w_k # [n_seq, n_embd] @ [n_embd, n_embd] -> [n_seq, n_embd]\n",
        "    k = x @ w_q # [n_seq, n_embd] @ [n_embd, n_embd] -> [n_seq, n_embd]\n",
        "    v = x @ w_v # [n_seq, n_embd] @ [n_embd, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # perform self attention\n",
        "    x = attention(q, k, v) # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # out projection\n",
        "    x = x @ w_proj # [n_seq, n_embd] @ [n_embd, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "# concatenate the parameters\n",
        "def self_attention_3(x, w_fc, w_proj): # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    # qkv projections\n",
        "    x = x @ w_fc # [n_seq, n_embd] @ [n_embd, 3*n_embd] -> [n_seq, 3*n_embd]\n",
        "\n",
        "    # split into qkv\n",
        "    q, k, v = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -> 3 of [n_seq, n_embd]\n",
        "\n",
        "    # perform self attention\n",
        "    x = attention(q, k, v) # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # out projection\n",
        "    x = x @ w_proj # [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "# add bias to linear projections\n",
        "def self_attention(x, c_attn, c_proj): # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    # qkv projections\n",
        "    x = linear(x, **c_attn) # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
        "\n",
        "    # split into qkv\n",
        "    q, k, v = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -> 3 of [n_seq, n_embd]\n",
        "\n",
        "    # perform self attention\n",
        "    x = attention(q, k, v) # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # out projection\n",
        "    x = linear(x, **c_proj) # [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "ao1uCo8JyR1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Causal\n",
        "Add attention masks to prevent former tokens from seeing latter tokens. "
      ],
      "metadata": {
        "id": "1194cv2-zUDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An example mask of 5*5\n",
        "# 0 -1e10   -1e10   -1e10   -1e10\n",
        "# 0   0   -1e10   -1e10   -1e10\n",
        "# 0   0     0   -1e10   -1e10\n",
        "# 0   0     0     0   -1e10\n",
        "# 0   0     0     0     0"
      ],
      "metadata": {
        "id": "1wOeEKUz16sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(q, k, v, mask):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v]\n",
        "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n",
        "\n",
        "def causal_self_attention(x, c_attn, c_proj): # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    # qkv projections\n",
        "    x = linear(x, **c_attn) # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
        "\n",
        "    # split into qkv\n",
        "    q, k, v = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -> 3 of [n_seq, n_embd]\n",
        "\n",
        "    # causal mask to hide future inputs from being attended to\n",
        "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n",
        "\n",
        "    # perform causal self attention\n",
        "    x = attention(q, k, v, causal_mask) # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # out projection\n",
        "    x = linear(x, **c_proj) # [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "Br0QMPSZz3C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Multi-head"
      ],
      "metadata": {
        "id": "eet4IdRy2pOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mha(x, c_attn, c_proj, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    # qkv projection\n",
        "    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
        "\n",
        "    # split into qkv\n",
        "    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> [3, n_seq, n_embd]\n",
        "\n",
        "    # split into heads\n",
        "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n",
        "\n",
        "    # causal mask to hide future inputs from being attended to\n",
        "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n",
        "\n",
        "    # perform attention over each head\n",
        "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]  # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n",
        "\n",
        "    # merge heads\n",
        "    x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
        "\n",
        "    # out projection\n",
        "    x = linear(x, **c_proj)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "zCc0UV2K2vR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we can load GPT-2 parameters to our architecture to conduct inference"
      ],
      "metadata": {
        "id": "Fjn-ks20g08Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some backend code to load GPT-2 tokenizer and parameters"
      ],
      "metadata": {
        "id": "6JVcklxCiRpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Byte pair encoding utilities.\n",
        "Copied from: https://github.com/openai/gpt-2/blob/master/src/encoder.py.\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import regex as re\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a significant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8 + n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, encoder, bpe_merges, errors=\"replace\"):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.errors = errors  # how to handle errors in decoding\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
        "        self.cache = {}\n",
        "\n",
        "        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
        "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\\\p{L}+| ?\\\\p{N}+| ?[^\\s\\\\p{L}\\\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
        "                    new_word.append(first + second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = \" \".join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = \"\".join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n",
        "        return text\n",
        "\n",
        "\n",
        "def get_encoder(model_name, models_dir):\n",
        "    with open(os.path.join(models_dir, model_name, \"encoder.json\"), \"r\") as f:\n",
        "        encoder = json.load(f)\n",
        "    with open(os.path.join(models_dir, model_name, \"vocab.bpe\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        bpe_data = f.read()\n",
        "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n",
        "    return Encoder(encoder=encoder, bpe_merges=bpe_merges)"
      ],
      "metadata": {
        "id": "HsP42_lNiifi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def download_gpt2_files(model_size, model_dir):\n",
        "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
        "    for filename in [\n",
        "        \"checkpoint\",\n",
        "        \"encoder.json\",\n",
        "        \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\",\n",
        "        \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\",\n",
        "        \"vocab.bpe\",\n",
        "    ]:\n",
        "        url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "        r = requests.get(f\"{url}/{model_size}/{filename}\", stream=True)\n",
        "        r.raise_for_status()\n",
        "\n",
        "        with open(os.path.join(model_dir, filename), \"wb\") as f:\n",
        "            file_size = int(r.headers[\"content-length\"])\n",
        "            chunk_size = 1000\n",
        "            with tqdm(\n",
        "                ncols=100,\n",
        "                desc=\"Fetching \" + filename,\n",
        "                total=file_size,\n",
        "                unit_scale=True,\n",
        "                unit=\"b\",\n",
        "            ) as pbar:\n",
        "                # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
        "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(chunk_size)\n",
        "\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams):\n",
        "    def set_in_nested_dict(d, keys, val):\n",
        "        if not keys:\n",
        "            return val\n",
        "        if keys[0] not in d:\n",
        "            d[keys[0]] = {}\n",
        "        d[keys[0]] = set_in_nested_dict(d[keys[0]], keys[1:], val)\n",
        "        return d\n",
        "\n",
        "    params = {\"blocks\": [{} for _ in range(hparams[\"n_layer\"])]}\n",
        "    for name, _ in tf.train.list_variables(tf_ckpt_path):\n",
        "        array = np.squeeze(tf.train.load_variable(tf_ckpt_path, name))\n",
        "        name = name[len(\"model/\") :]\n",
        "        if name.startswith(\"h\"):\n",
        "            m = re.match(r\"h([0-9]+)/(.*)\", name)\n",
        "            n = int(m[1])\n",
        "            sub_name = m[2]\n",
        "            set_in_nested_dict(params[\"blocks\"][n], sub_name.split(\"/\"), array)\n",
        "        else:\n",
        "            set_in_nested_dict(params, name.split(\"/\"), array)\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "def load_encoder_hparams_and_params(model_size, models_dir):\n",
        "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
        "\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    if not tf_ckpt_path:  # download files if necessary\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        download_gpt2_files(model_size, model_dir)\n",
        "        tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "\n",
        "    encoder = get_encoder(model_size, models_dir)\n",
        "    hparams = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams)\n",
        "\n",
        "    return encoder, hparams, params"
      ],
      "metadata": {
        "id": "vjlUlhMhhy-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for generation"
      ],
      "metadata": {
        "id": "ea-hWIu3i3lI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important Note: Our current implementation requires us to specify the exact number of tokens we'd like to generate ahead of time. This is not a very good approach as our generations end up being too long, too short, or cutoff mid-sentence.\n",
        "\n",
        "To resolve this, we can introduce a special end of sentence (EOS) token.\n",
        "\n",
        "GPT-2 was not pre-trained with an EOS token, so we can't use this approach in our code, but most LLMs nowadays use an EOS token."
      ],
      "metadata": {
        "id": "FbvMqw6NsSjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    for _ in tqdm(range(n_tokens_to_generate), \"generating\"):  # auto-regressive decode loop\n",
        "        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
        "        next_id = np.argmax(logits[-1])  # greedy sampling\n",
        "        inputs.append(int(next_id))  # append prediction to input\n",
        "\n",
        "    return inputs[len(inputs) - n_tokens_to_generate :]  # only return generated ids"
      ],
      "metadata": {
        "id": "bLU4ua-eicJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_tokens_to_generate = 40\n",
        "model_size = \"124M\"\n",
        "models_dir = \".\"\n",
        "prompt = \"Alan Turing theorized that computers would one day become\"\n",
        "\n",
        "# load encoder, hparams, and params from the released open-ai gpt-2 files\n",
        "encoder, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)"
      ],
      "metadata": {
        "id": "DY1CmKdAjAZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try the encoder"
      ],
      "metadata": {
        "id": "v9X3R2WAobTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = encoder.encode(\"Not all heroes wear capes.\")\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHJfgbqfod_F",
        "outputId": "90d6edcd-5d7f-4ba1-c340-0fe8cd676ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[313, 477, 10281, 5806, 1275, 274, 13]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q2O20FlQoh3Z",
        "outputId": "2242f3d9-c6ed-4327-e179-7e052839ef6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ot all heroes wear caes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the hyperparamteres"
      ],
      "metadata": {
        "id": "lDC7NnK9olB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hparams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOIEyh8AorFC",
        "outputId": "7889d995-0bd4-4768-87b5-7ac21f7999d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Structure of the parameters"
      ],
      "metadata": {
        "id": "iibA9g6rotLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shape_tree(d):\n",
        "  if isinstance(d, np.ndarray):\n",
        "    return list(d.shape)\n",
        "  elif isinstance(d, list):\n",
        "    return [shape_tree(v) for v in d]\n",
        "  elif isinstance(d, dict):\n",
        "    return {k: shape_tree(v) for k, v in d.items()}\n",
        "  else:\n",
        "    ValueError(\"uh oh\")\n",
        "\n",
        "shape_tree(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1gyQVddosxP",
        "outputId": "a91a3556-032f-4acc-f4d6-878e0d44e6f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'blocks': [{'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
              "    'c_proj': {'b': [768], 'w': [768, 768]}},\n",
              "   'ln_1': {'b': [768], 'g': [768]},\n",
              "   'ln_2': {'b': [768], 'g': [768]},\n",
              "   'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
              "    'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
              "  {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
              "    'c_proj': {'b': [768], 'w': [768, 768]}},\n",
              "   'ln_1': {'b': [768], 'g': [768]},\n",
              "   'ln_2': {'b': [768], 'g': [768]},\n",
              "   'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
              "    'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
              "  {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
              "    'c_proj': {'b': [768], 'w': [768, 768]}},\n",
              "   'ln_1': {'b': [768], 'g': [768]},\n",
              "   'ln_2': {'b': [768], 'g': [768]},\n",
              "   'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
              "    'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
              "  {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
              "    'c_proj': {'b': [768], 'w': [768, 768]}},\n",
              "   'ln_1': {'b': [768], 'g': [768]},\n",
              "   'ln_2': {'b': [768], 'g': [768]},\n",
              "   'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
              "    'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
              "  {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
              "    'c_proj': {'b': [768], 'w': [768, 768]}},\n",
              "   'ln_1': {'b': [768], 'g': [768]},\n",
              "   'ln_2': {'b': [768], 'g': [768]},\n",
              "   'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
              "    'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
              "  {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
              "    'c_proj': {'b': [768], 'w': [768, 768]}},\n",
              "   'ln_1': {'b': [768], 'g': [768]},\n",
              "   'ln_2': {'b': [768], 'g': [768]},\n",
              "   'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
              "    'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
              "  {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
              "    'c_proj': {'b': [768], 'w': [768, 768]}},\n",
              "   'ln_1': {'b': [768], 'g': [768]},\n",
              "   'ln_2': {'b': [768], 'g': [768]},\n",
              "   'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
              "    'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
              "  {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
              "    'c_proj': {'b': [768], 'w': [768, 768]}},\n",
              "   'ln_1': {'b': [768], 'g': [768]},\n",
              "   'ln_2': {'b': [768], 'g': [768]},\n",
              "   'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
              "    'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
              "  {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
              "    'c_proj': {'b': [768], 'w': [768, 768]}},\n",
              "   'ln_1': {'b': [768], 'g': [768]},\n",
              "   'ln_2': {'b': [768], 'g': [768]},\n",
              "   'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
              "    'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
              "  {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
              "    'c_proj': {'b': [768], 'w': [768, 768]}},\n",
              "   'ln_1': {'b': [768], 'g': [768]},\n",
              "   'ln_2': {'b': [768], 'g': [768]},\n",
              "   'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
              "    'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
              "  {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
              "    'c_proj': {'b': [768], 'w': [768, 768]}},\n",
              "   'ln_1': {'b': [768], 'g': [768]},\n",
              "   'ln_2': {'b': [768], 'g': [768]},\n",
              "   'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
              "    'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
              "  {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
              "    'c_proj': {'b': [768], 'w': [768, 768]}},\n",
              "   'ln_1': {'b': [768], 'g': [768]},\n",
              "   'ln_2': {'b': [768], 'g': [768]},\n",
              "   'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
              "    'c_proj': {'b': [768], 'w': [3072, 768]}}}],\n",
              " 'ln_f': {'b': [768], 'g': [768]},\n",
              " 'wpe': [1024, 768],\n",
              " 'wte': [50257, 768]}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference using our architecture + loaded parameters"
      ],
      "metadata": {
        "id": "u7Y9QqSWpWar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the input string using the BPE tokenizer\n",
        "input_ids = encoder.encode(prompt)\n",
        "\n",
        "# make sure we are not surpassing the max sequence length of our model\n",
        "assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
        "\n",
        "# generate output ids\n",
        "output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
        "\n",
        "# decode the ids back into a string\n",
        "output_text = encoder.decode(output_ids)\n",
        "\n",
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQM0zSfjpBjn",
        "outputId": "96280ece-de06-44c0-8f43-be3b98495c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "generating: 100%|██████████| 40/40 [00:14<00:00,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \"computational\" and \"computational\" in the same way that humans become \"computational\" and \"computational\" in the same way that humans become \"computational\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}